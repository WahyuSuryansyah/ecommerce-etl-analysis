{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6417c4db-bc7e-4651-ada6-cc5b8fdd4baa",
   "metadata": {},
   "source": [
    "# ETL Pipeline for E-commerce Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8644231b-3d4f-4ef2-a916-387042dc8224",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This ETL (Extract, Transform, Load) process is for an **e-commerce dataset** comprising several key entities: Users, Products, Categories, Orders, Payments, Reviews, and OrderItems. The data was sourced from CSV files, cleaned, transformed, and loaded into a **MySQL relational database** to support further data analysis and reporting.\n",
    "\n",
    "The **ETL pipeline** consists of 3 main stages:\n",
    "\n",
    "1. **Extract:** Retrieve raw data from multiple CSV sources.\n",
    "2. **Transform:** Clean, reformat, handle missing values, establish relationships via foreign keys, and ensure data consistency across tables.\n",
    "3. **Load:** Insert the processed data into a MySQL database following the appropriate schema design.\n",
    "\n",
    "ðŸ“Š **Notebook Structure**\n",
    "\n",
    "This notebook is organized into the following sections:\n",
    "\n",
    "1. Extract\n",
    "2. Transform\n",
    "3. Load\n",
    "4. **Challenges and Solutions:** Key issues encountered during the ETL process and how they were resolved.\n",
    "5. **Takeaways:** Reflections and potential next steps for analysis or improvement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9562cbe5-ecfc-4d7d-b794-4db55e35d75a",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b532d68f-7d93-481c-be5c-02316965d7d4",
   "metadata": {},
   "source": [
    "Imported datasets from several CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9168ba76-99e1-42ea-bf70-c198b034a622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-9.3.0-cp313-cp313-win_amd64.whl.metadata (7.7 kB)\n",
      "Downloading mysql_connector_python-9.3.0-cp313-cp313-win_amd64.whl (16.4 MB)\n",
      "   ---------------------------------------- 0.0/16.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/16.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/16.4 MB 2.9 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 1.6/16.4 MB 3.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 2.4/16.4 MB 3.3 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 3.4/16.4 MB 3.7 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 4.2/16.4 MB 3.9 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 5.5/16.4 MB 4.2 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 6.8/16.4 MB 4.5 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 8.1/16.4 MB 4.7 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.4/16.4 MB 4.9 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 11.3/16.4 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.8/16.4 MB 5.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.7/16.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.4/16.4 MB 6.0 MB/s eta 0:00:00\n",
      "Installing collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "07ee0779-a967-4b23-8a15-e8fdd4a3eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "from io import StringIO\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0694697-502d-41ae-b10f-11a0e4dc7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MySQL db\n",
    "conn = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='***',\n",
    "    database='ecommerce_db'\n",
    ")\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "88a812d8-8660-4303-b5be-518a3383d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from CSV files\n",
    "\n",
    "# users data\n",
    "users_df = pd.read_csv('~/OneDrive/Documents/ecommerce_etl/users_data.csv',\n",
    "    encoding='utf-8-sig',\n",
    "    sep = \",\"\n",
    ")\n",
    "\n",
    "# users_df = pd.read_csv('~/OneDrive/Documents/ecommerce_etl/users_data.csv', header=None)\n",
    "# users_df = users_df[0].str.split(',', expand=True)\n",
    "# users_df.columns = ['UserName', 'Email', 'Password', 'Address', 'Extra']\n",
    "\n",
    "# product data\n",
    "products_df = pd.read_csv('~/OneDrive/Documents/ecommerce_etl/products_list.csv')\n",
    "\n",
    "# category data\n",
    "categories_df = pd.read_csv('~/OneDrive/Documents/ecommerce_etl/categories_data.csv', quotechar='\"')\n",
    "\n",
    "# order data\n",
    "orders_df = pd.read_csv('~/OneDrive/Documents/ecommerce_etl/orders_data.csv')\n",
    "\n",
    "# orderitem data\n",
    "orderItems_df = pd.read_csv('~/OneDrive/Documents/ecommerce_etl/orderitems_data.csv')\n",
    "\n",
    "# payment data\n",
    "payments_df = pd.read_csv('~/OneDrive/Documents/ecommerce_etl/payments_data.csv')\n",
    "\n",
    "# reviews_df = pd.read_csv(\n",
    "#     '~/OneDrive/Documents/ecommerce_etl/reviews_data.csv',\n",
    "#     sep=',',\n",
    "#     quotechar='\"',\n",
    "#     encoding='utf-8'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae94c6c-2aae-41e6-967c-1b26990db70e",
   "metadata": {},
   "source": [
    "Check how the users data is parsed, because somehow tha comma in address data is considered as a delimiter \n",
    "|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ed8d9dc9-81b2-4e92-9356-60934c4d28b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UserName', 'Email', 'Password', 'Address']\n",
      "['Andi Pratama,andi.pratama@email.com,hashedpass001,\"Jl. Merdeka No.10, Jakarta Pusat\"']\n",
      "['Siti Aminah,siti.aminah@email.com,hashedpass002,\"Jl. Raya Bogor No.45, Depok\"']\n",
      "['Rizky Hidayat,rizky.hidayat@email.com,hashedpass003,\"Jl. Ahmad Yani No.21, Bandung\"']\n",
      "['Dewi Kartika,dewi.kartika@email.com,hashedpass004,\"Jl. Malioboro No.7, Yogyakarta\"']\n",
      "['Budi Santoso,budi.santoso@email.com,hashedpass005,\"Jl. Diponegoro No.99, Surabaya\"']\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.expanduser('~/OneDrive/Documents/ecommerce_etl/users_data.csv')\n",
    "\n",
    "with open(file_path, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    for row in reader:\n",
    "        print(repr(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1669a9-2554-4b22-83aa-9a2274caff1e",
   "metadata": {},
   "source": [
    "As seen from data above, there are 4 columns but tha data/records are divided into 5 columns because the comma in address data is considered as delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0574d3b8-f216-413e-93df-8a272b55399a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        UserName                    Email       Password  \\\n",
      "0   Andi Pratama   andi.pratama@email.com  hashedpass001   \n",
      "1    Siti Aminah    siti.aminah@email.com  hashedpass002   \n",
      "2  Rizky Hidayat  rizky.hidayat@email.com  hashedpass003   \n",
      "3   Dewi Kartika   dewi.kartika@email.com  hashedpass004   \n",
      "4   Budi Santoso   budi.santoso@email.com  hashedpass005   \n",
      "\n",
      "                            Address  \n",
      "0  Jl. Merdeka No.10, Jakarta Pusat  \n",
      "1       Jl. Raya Bogor No.45, Depok  \n",
      "2     Jl. Ahmad Yani No.21, Bandung  \n",
      "3    Jl. Malioboro No.7, Yogyakarta  \n",
      "4    Jl. Diponegoro No.99, Surabaya  \n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.expanduser('~/OneDrive/Documents/ecommerce_etl/users_data.csv')\n",
    "\n",
    "def parse_line(line):\n",
    "    reader = csv.reader(StringIO(line), delimiter=',', quotechar='\"')\n",
    "    return next(reader)\n",
    "\n",
    "df_raw = pd.read_csv(file_path, header=None, encoding='utf-8-sig', engine='python')\n",
    "parsed_rows = df_raw[0].apply(parse_line)\n",
    "users_df = pd.DataFrame(parsed_rows.tolist(), columns=['UserName', 'Email', 'Password', 'Address'])\n",
    "\n",
    "users_df = users_df[users_df['UserName'] != 'UserName'].reset_index(drop=True)\n",
    "print(users_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a2783-2b81-40a1-ba57-1b117f15d3f5",
   "metadata": {},
   "source": [
    "Finally, the address data in users dataset is parsed correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36350ea0-4617-43eb-9382-8f11736aecce",
   "metadata": {},
   "source": [
    "The same problem occurs in the **Reviews and Categories DataFrames** because the ReviewText and Description columns contain commas. As a result, all data records appear in the first column, while the remaining columns contain **NaN values**.\n",
    "\n",
    "**Below is the solution** I used to handle this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "06a2128d-1a04-4494-a953-fdb1bfaede08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ProductID UserID Rating                                         ReviewText\n",
      "0         1      2      5  Laptop ini sangat cepat dan cocok untuk kerja ...\n",
      "1         2      1      4                           Headphone nyaman dipakai\n",
      "2         3      5      5  Smartwatch-nya membantu banget buat lacak olah...\n",
      "3         4      3      4                                        Jaket keren\n",
      "4         5      4      3                                       Kaos lumayan\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.expanduser('~/OneDrive/Documents/ecommerce_etl/reviews_data.csv')\n",
    "\n",
    "with open(file_path, encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# regex untuk memisahkan data per baris\n",
    "records = re.findall(r'(\\d+,\\d+,\\d+,\".*?\")(?=\\s*\\d+,\\d+,\\d+,\"|$)', content, re.DOTALL)\n",
    "\n",
    "header = ['ProductID', 'UserID', 'Rating', 'ReviewText']\n",
    "parsed_rows = []\n",
    "\n",
    "for record in records:\n",
    "    reader = csv.reader(StringIO(record), delimiter=',', quotechar='\"')\n",
    "    parsed = next(reader)\n",
    "    parsed = parsed[:4]\n",
    "    parsed_rows.append(parsed)\n",
    "\n",
    "reviews_df = pd.DataFrame(parsed_rows, columns=header)\n",
    "print(reviews_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "dcf70818-4c7e-427c-8ec6-f73d3fb1c325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CategoryName</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Electronics</td>\n",
       "      <td>Produk elektronik seperti HP, laptop, smartwat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fashion</td>\n",
       "      <td>Pakaian, sepatu, tas, dan aksesoris untuk pria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home Appliances</td>\n",
       "      <td>Peralatan rumah tangga seperti blender, vacuum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Books</td>\n",
       "      <td>Buku pendidikan, pengembangan diri, teknologi,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CategoryName                                        Description\n",
       "0      Electronics  Produk elektronik seperti HP, laptop, smartwat...\n",
       "1          Fashion  Pakaian, sepatu, tas, dan aksesoris untuk pria...\n",
       "2  Home Appliances  Peralatan rumah tangga seperti blender, vacuum...\n",
       "3            Books  Buku pendidikan, pengembangan diri, teknologi,..."
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = pd.read_csv('~/OneDrive/Documents/ecommerce_etl/categories_data.csv', header=None)\n",
    "\n",
    "# Split column by first comma\n",
    "categories_df = raw_df[0].str.split(',', n=1, expand=True)\n",
    "\n",
    "# Rename column\n",
    "categories_df.columns = ['CategoryName', 'Description']\n",
    "\n",
    "# Remove \" from description data\n",
    "categories_df['Description'] = categories_df['Description'].str.strip('\"')\n",
    "\n",
    "# Drop header\n",
    "categories_df = categories_df[categories_df['CategoryName'] != 'CategoryName'].reset_index(drop=True)\n",
    "\n",
    "categories_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae748c-944b-4b57-8c94-c1e64535eca8",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c3f60f-a7e6-4493-bdbf-2acf982b093b",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9bd343-52f9-43ef-be13-11cfa91a1b89",
   "metadata": {},
   "source": [
    "Key transformation steps performed:\n",
    "\n",
    "1. **Data Cleaning**\n",
    "    - Removed NaN values and unnecessary headers (e.g., duplicate headers in CSV files).\n",
    "    - Dropped rows with incomplete references, such as missing price in OrderItems.\n",
    "\n",
    "2. **Data Type Conversion**\n",
    "\n",
    "\n",
    "   Converted fields like Price, StockQuantity, Rating, and foreign keys (UserID, ProductID) to appropriate numeric types.\n",
    "\n",
    "4. **Primary Key Generation**\n",
    "\n",
    "\n",
    "   Assigned unique identifiers (Primary Keys) to each entity like UserID, ProductID, CategoryID, etc.\n",
    "\n",
    "6. **Mapping & Normalization**\n",
    "   -  Established correct foreign key relationships between tables:\n",
    "    - Mapped ProductID to OrderItems and Reviews.\n",
    "    - Mapped UserID to Orders and Reviews.\n",
    "    - Mapped OrderID to Payments and OrderItems.\n",
    "\n",
    "7. **Data Enrichment**\n",
    "    - Extracted City from the Address field in the Users dataset.\n",
    "    - Added DateRegistered for Users and DateAdded for Products using the current date.\n",
    "\n",
    "8. **Calculating Aggregates**\n",
    "    - Calculated TotalAmount for each order by summing up the total price of its OrderItems.\n",
    "    - Assigned Amount in the Payments dataset based on the corresponding Order's TotalAmount.\n",
    "\n",
    "9. **Standardizing Date and Rating Formats**\n",
    "    - Standardized date fields to consistent formats (YYYY-MM-DD).\n",
    "    - Ensured ratings are integers within a valid range (1 to 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7b10e9d2-1d7e-4f4b-9fc0-0dc44bcb8778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Users Dataset ====================\n",
      "Shape: (5, 4)\n",
      "\n",
      "-- Sample Data:\n",
      "        UserName                    Email       Password  \\\n",
      "0   Andi Pratama   andi.pratama@email.com  hashedpass001   \n",
      "1    Siti Aminah    siti.aminah@email.com  hashedpass002   \n",
      "2  Rizky Hidayat  rizky.hidayat@email.com  hashedpass003   \n",
      "3   Dewi Kartika   dewi.kartika@email.com  hashedpass004   \n",
      "4   Budi Santoso   budi.santoso@email.com  hashedpass005   \n",
      "\n",
      "                            Address  \n",
      "0  Jl. Merdeka No.10, Jakarta Pusat  \n",
      "1       Jl. Raya Bogor No.45, Depok  \n",
      "2     Jl. Ahmad Yani No.21, Bandung  \n",
      "3    Jl. Malioboro No.7, Yogyakarta  \n",
      "4    Jl. Diponegoro No.99, Surabaya  \n",
      "\n",
      "-- Missing Values:\n",
      "UserName    0\n",
      "Email       0\n",
      "Password    0\n",
      "Address     0\n",
      "dtype: int64\n",
      "\n",
      "-- Data Types:\n",
      "UserName    object\n",
      "Email       object\n",
      "Password    object\n",
      "Address     object\n",
      "dtype: object\n",
      "\n",
      "-- Descriptive Statistics (Numeric):\n",
      "            UserName                   Email       Password  \\\n",
      "count              5                       5              5   \n",
      "unique             5                       5              5   \n",
      "top     Andi Pratama  andi.pratama@email.com  hashedpass001   \n",
      "freq               1                       1              1   \n",
      "\n",
      "                                 Address  \n",
      "count                                  5  \n",
      "unique                                 5  \n",
      "top     Jl. Merdeka No.10, Jakarta Pusat  \n",
      "freq                                   1  \n",
      "\n",
      "==================== Products Dataset ====================\n",
      "Shape: (10, 5)\n",
      "\n",
      "-- Sample Data:\n",
      "           ProductName                                     Description  \\\n",
      "0     Laptop Ultrabook               Slim ultrabook with Intel Core i7   \n",
      "1  Wireless Headphones  Noise-cancelling headphones with Bluetooth 5.0   \n",
      "2           Smartwatch    Fitness tracking and notification smartwatch   \n",
      "3       Leather Jacket         Genuine leather jacket for winter style   \n",
      "4       Casual T-Shirt         Soft cotton t-shirt with printed design   \n",
      "\n",
      "      Price  StockQuantity  Category  \n",
      "0  15500000             30         1  \n",
      "1   1800000             75         1  \n",
      "2   2300000             50         1  \n",
      "3   1250000             20         2  \n",
      "4     85000            120         2  \n",
      "\n",
      "-- Missing Values:\n",
      "ProductName      0\n",
      "Description      0\n",
      "Price            0\n",
      "StockQuantity    0\n",
      "Category         0\n",
      "dtype: int64\n",
      "\n",
      "-- Data Types:\n",
      "ProductName      object\n",
      "Description      object\n",
      "Price             int64\n",
      "StockQuantity     int64\n",
      "Category          int64\n",
      "dtype: object\n",
      "\n",
      "-- Descriptive Statistics (Numeric):\n",
      "              Price  StockQuantity   Category\n",
      "count  1.000000e+01      10.000000  10.000000\n",
      "mean   2.358000e+06      61.000000   2.200000\n",
      "std    4.673411e+06      32.386554   1.032796\n",
      "min    8.500000e+04      20.000000   1.000000\n",
      "25%    3.600000e+05      36.250000   1.250000\n",
      "50%    8.500000e+05      55.000000   2.000000\n",
      "75%    1.662500e+06      78.750000   3.000000\n",
      "max    1.550000e+07     120.000000   4.000000\n",
      "\n",
      "==================== Categories Dataset ====================\n",
      "Shape: (4, 2)\n",
      "\n",
      "-- Sample Data:\n",
      "      CategoryName                                        Description\n",
      "0      Electronics  Produk elektronik seperti HP, laptop, smartwat...\n",
      "1          Fashion  Pakaian, sepatu, tas, dan aksesoris untuk pria...\n",
      "2  Home Appliances  Peralatan rumah tangga seperti blender, vacuum...\n",
      "3            Books  Buku pendidikan, pengembangan diri, teknologi,...\n",
      "\n",
      "-- Missing Values:\n",
      "CategoryName    0\n",
      "Description     0\n",
      "dtype: int64\n",
      "\n",
      "-- Data Types:\n",
      "CategoryName    object\n",
      "Description     object\n",
      "dtype: object\n",
      "\n",
      "-- Descriptive Statistics (Numeric):\n",
      "       CategoryName                                        Description\n",
      "count             4                                                  4\n",
      "unique            4                                                  4\n",
      "top     Electronics  Produk elektronik seperti HP, laptop, smartwat...\n",
      "freq              1                                                  1\n",
      "\n",
      "==================== Orders Dataset ====================\n",
      "Shape: (5, 2)\n",
      "\n",
      "-- Sample Data:\n",
      "   UserID   OrderDate\n",
      "0       1  2025-07-01\n",
      "1       2  2025-07-02\n",
      "2       3  2025-07-03\n",
      "3       4  2025-07-03\n",
      "4       5  2025-07-04\n",
      "\n",
      "-- Missing Values:\n",
      "UserID       0\n",
      "OrderDate    0\n",
      "dtype: int64\n",
      "\n",
      "-- Data Types:\n",
      "UserID        int64\n",
      "OrderDate    object\n",
      "dtype: object\n",
      "\n",
      "-- Descriptive Statistics (Numeric):\n",
      "         UserID\n",
      "count  5.000000\n",
      "mean   3.000000\n",
      "std    1.581139\n",
      "min    1.000000\n",
      "25%    2.000000\n",
      "50%    3.000000\n",
      "75%    4.000000\n",
      "max    5.000000\n",
      "\n",
      "==================== OrderItems Dataset ====================\n",
      "Shape: (9, 3)\n",
      "\n",
      "-- Sample Data:\n",
      "   OrderID  ProductID  Quantity\n",
      "0        1          5         2\n",
      "1        1          1         1\n",
      "2        3          6         1\n",
      "3        2         13         1\n",
      "4        4         10         1\n",
      "\n",
      "-- Missing Values:\n",
      "OrderID      0\n",
      "ProductID    0\n",
      "Quantity     0\n",
      "dtype: int64\n",
      "\n",
      "-- Data Types:\n",
      "OrderID      int64\n",
      "ProductID    int64\n",
      "Quantity     int64\n",
      "dtype: object\n",
      "\n",
      "-- Descriptive Statistics (Numeric):\n",
      "        OrderID  ProductID  Quantity\n",
      "count  9.000000   9.000000  9.000000\n",
      "mean   3.000000   7.111111  1.222222\n",
      "std    1.414214   4.484541  0.440959\n",
      "min    1.000000   1.000000  1.000000\n",
      "25%    2.000000   4.000000  1.000000\n",
      "50%    3.000000   6.000000  1.000000\n",
      "75%    4.000000  10.000000  1.000000\n",
      "max    5.000000  14.000000  2.000000\n",
      "\n",
      "==================== Payments Dataset ====================\n",
      "Shape: (5, 3)\n",
      "\n",
      "-- Sample Data:\n",
      "   OrderID     PaymentMethod PaymentDate\n",
      "0        1        Debit Card  2025-07-01\n",
      "1        2       Credit Card  2025-07-02\n",
      "2        3  Cash on Delivery  2025-07-03\n",
      "3        4         ShopeePay  2025-07-03\n",
      "4        5              DANA  2025-07-04\n",
      "\n",
      "-- Missing Values:\n",
      "OrderID          0\n",
      "PaymentMethod    0\n",
      "PaymentDate      0\n",
      "dtype: int64\n",
      "\n",
      "-- Data Types:\n",
      "OrderID           int64\n",
      "PaymentMethod    object\n",
      "PaymentDate      object\n",
      "dtype: object\n",
      "\n",
      "-- Descriptive Statistics (Numeric):\n",
      "        OrderID\n",
      "count  5.000000\n",
      "mean   3.000000\n",
      "std    1.581139\n",
      "min    1.000000\n",
      "25%    2.000000\n",
      "50%    3.000000\n",
      "75%    4.000000\n",
      "max    5.000000\n",
      "\n",
      "==================== Reviews Dataset ====================\n",
      "Shape: (5, 4)\n",
      "\n",
      "-- Sample Data:\n",
      "  ProductID UserID Rating                                         ReviewText\n",
      "0         1      2      5  Laptop ini sangat cepat dan cocok untuk kerja ...\n",
      "1         2      1      4                           Headphone nyaman dipakai\n",
      "2         3      5      5  Smartwatch-nya membantu banget buat lacak olah...\n",
      "3         4      3      4                                        Jaket keren\n",
      "4         5      4      3                                       Kaos lumayan\n",
      "\n",
      "-- Missing Values:\n",
      "ProductID     0\n",
      "UserID        0\n",
      "Rating        0\n",
      "ReviewText    0\n",
      "dtype: int64\n",
      "\n",
      "-- Data Types:\n",
      "ProductID     object\n",
      "UserID        object\n",
      "Rating        object\n",
      "ReviewText    object\n",
      "dtype: object\n",
      "\n",
      "-- Descriptive Statistics (Numeric):\n",
      "       ProductID UserID Rating  \\\n",
      "count          5      5      5   \n",
      "unique         5      5      3   \n",
      "top            1      2      5   \n",
      "freq           1      1      2   \n",
      "\n",
      "                                               ReviewText  \n",
      "count                                                   5  \n",
      "unique                                                  5  \n",
      "top     Laptop ini sangat cepat dan cocok untuk kerja ...  \n",
      "freq                                                    1  \n"
     ]
    }
   ],
   "source": [
    "# list dataframe to be analyzed\n",
    "dataframes = {\n",
    "    \"Users\": users_df,\n",
    "    \"Products\": products_df,\n",
    "    \"Categories\": categories_df,\n",
    "    \"Orders\": orders_df,\n",
    "    \"OrderItems\": orderItems_df,\n",
    "    \"Payments\": payments_df,\n",
    "    \"Reviews\": reviews_df\n",
    "}\n",
    "\n",
    "# Loop EDA per dataset\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n==================== {name} Dataset ====================\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"\\n-- Sample Data:\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\n-- Missing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    print(\"\\n-- Data Types:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "    print(\"\\n-- Descriptive Statistics (Numeric):\")\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6a2825be-059f-4254-8fa5-05ed3aa5245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate current timestamp string\n",
    "def current_timestamp():\n",
    "    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# ================== Generate Primary Keys ==================\n",
    "users_df['UserID'] = range(1, len(users_df) + 1)\n",
    "products_df['ProductID'] = range(1, len(products_df) + 1)\n",
    "categories_df['CategoryID'] = range(1, len(categories_df) + 1)\n",
    "orders_df['OrderID'] = range(1, len(orders_df) + 1)\n",
    "orderItems_df['OrderItemID'] = range(1, len(orderItems_df) + 1)\n",
    "payments_df['PaymentID'] = range(1, len(payments_df) + 1)\n",
    "reviews_df['ReviewID'] = range(1, len(reviews_df) + 1)\n",
    "\n",
    "# ================== USERS ==================\n",
    "# Extract City from the 'Address' column\n",
    "users_df['City'] = users_df['Address'].str.extract(r',\\s*(\\w+(?:\\s\\w+)*)$')\n",
    "\n",
    "# Add 'DateRegistered' as today (date only, no time)\n",
    "users_df['DateRegistered'] = pd.to_datetime('today').normalize()\n",
    "\n",
    "\n",
    "# ================== PRODUCTS ==================\n",
    "# Add today's date as 'DateAdded'\n",
    "products_df['DateAdded'] = pd.to_datetime('today').normalize()\n",
    "\n",
    "\n",
    "# ================== ORDERS ==================\n",
    "# Merge OrderItems with Products to get 'Price' per ProductID\n",
    "orderitems_with_price = orderItems_df.merge(products_df[['ProductID', 'Price']], on='ProductID', how='left')\n",
    "\n",
    "# Remove rows where price is missing (to avoid NaN in calculations)\n",
    "orderitems_with_price = orderitems_with_price.dropna(subset=['Price'])\n",
    "\n",
    "# Calculate 'TotalPrice' per item (Quantity * Price)\n",
    "orderitems_with_price['TotalPrice'] = orderitems_with_price['Quantity'] * orderitems_with_price['Price']\n",
    "\n",
    "# Aggregate total order amount per OrderID\n",
    "orders_total = orderitems_with_price.groupby('OrderID')['TotalPrice'].sum().reset_index().rename(columns={'TotalPrice': 'TotalAmount'})\n",
    "\n",
    "# Merge calculated TotalAmount into orders\n",
    "orders_df = orders_df.merge(orders_total, on='OrderID', how='left')\n",
    "orders_df = orders_df.dropna()\n",
    "\n",
    "\n",
    "# ================== ORDER ITEMS ==================\n",
    "# Merge to get 'Price' from products for each OrderItem\n",
    "orderItems_df = orderItems_df.merge(products_df[['ProductID', 'Price']], on='ProductID', how='left')\n",
    "\n",
    "orderItems_df = orderItems_df.dropna()\n",
    "\n",
    "# ================== REVIEWS ==================\n",
    "# Ensure data types for foreign keys and rating\n",
    "reviews_df['ProductID'] = reviews_df['ProductID'].astype(int)\n",
    "reviews_df['UserID'] = reviews_df['UserID'].astype(int)\n",
    "reviews_df['Rating'] = reviews_df['Rating'].astype(int)\n",
    "reviews_df = reviews_df[(reviews_df['Rating'] >= 1) & (reviews_df['Rating'] <= 5)]\n",
    "\n",
    "\n",
    "# ================== PAYMENTS ==================\n",
    "# Merge to get the TotalAmount (renamed as Amount) from Orders\n",
    "payments_df = payments_df.merge(orders_df[['OrderID', 'TotalAmount']], on='OrderID', how='left')\n",
    "payments_df.rename(columns={'TotalAmount': 'Amount'}, inplace=True)\n",
    "payments_df = payments_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "8e7cc232-3f61-463c-99bf-0631f425e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop primary key columns in each dataframe\n",
    "users_df = users_df.drop(columns=['UserID'])\n",
    "products_df = products_df.drop(columns=['ProductID'])\n",
    "categories_df = categories_df.drop(columns=['CategoryID'])\n",
    "orders_df = orders_df.drop(columns=['OrderID'])\n",
    "orderItems_df = orderItems_df.drop(columns=['OrderItemID'])\n",
    "payments_df = payments_df.drop(columns=['PaymentID'])\n",
    "reviews_df = reviews_df.drop(columns=['ReviewID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b329e393-ab6f-4f06-bfb5-4477295d0ae2",
   "metadata": {},
   "source": [
    "### Final Check after Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "512a06e1-003d-4194-9fa7-8d47f3a39bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Users Dataset ====================\n",
      "Shape: (5, 6)\n",
      "\n",
      "-- Sample Data:\n",
      "        UserName                    Email       Password  \\\n",
      "0   Andi Pratama   andi.pratama@email.com  hashedpass001   \n",
      "1    Siti Aminah    siti.aminah@email.com  hashedpass002   \n",
      "2  Rizky Hidayat  rizky.hidayat@email.com  hashedpass003   \n",
      "3   Dewi Kartika   dewi.kartika@email.com  hashedpass004   \n",
      "4   Budi Santoso   budi.santoso@email.com  hashedpass005   \n",
      "\n",
      "                            Address           City DateRegistered  \n",
      "0  Jl. Merdeka No.10, Jakarta Pusat  Jakarta Pusat     2025-07-22  \n",
      "1       Jl. Raya Bogor No.45, Depok          Depok     2025-07-22  \n",
      "2     Jl. Ahmad Yani No.21, Bandung        Bandung     2025-07-22  \n",
      "3    Jl. Malioboro No.7, Yogyakarta     Yogyakarta     2025-07-22  \n",
      "4    Jl. Diponegoro No.99, Surabaya       Surabaya     2025-07-22  \n",
      "\n",
      "-- Missing Values:\n",
      "UserName          0\n",
      "Email             0\n",
      "Password          0\n",
      "Address           0\n",
      "City              0\n",
      "DateRegistered    0\n",
      "dtype: int64\n",
      "\n",
      "==================== Products Dataset ====================\n",
      "Shape: (10, 6)\n",
      "\n",
      "-- Sample Data:\n",
      "           ProductName                                     Description  \\\n",
      "0     Laptop Ultrabook               Slim ultrabook with Intel Core i7   \n",
      "1  Wireless Headphones  Noise-cancelling headphones with Bluetooth 5.0   \n",
      "2           Smartwatch    Fitness tracking and notification smartwatch   \n",
      "3       Leather Jacket         Genuine leather jacket for winter style   \n",
      "4       Casual T-Shirt         Soft cotton t-shirt with printed design   \n",
      "\n",
      "      Price  StockQuantity  Category  DateAdded  \n",
      "0  15500000             30         1 2025-07-22  \n",
      "1   1800000             75         1 2025-07-22  \n",
      "2   2300000             50         1 2025-07-22  \n",
      "3   1250000             20         2 2025-07-22  \n",
      "4     85000            120         2 2025-07-22  \n",
      "\n",
      "-- Missing Values:\n",
      "ProductName      0\n",
      "Description      0\n",
      "Price            0\n",
      "StockQuantity    0\n",
      "Category         0\n",
      "DateAdded        0\n",
      "dtype: int64\n",
      "\n",
      "==================== Categories Dataset ====================\n",
      "Shape: (4, 2)\n",
      "\n",
      "-- Sample Data:\n",
      "      CategoryName                                        Description\n",
      "0      Electronics  Produk elektronik seperti HP, laptop, smartwat...\n",
      "1          Fashion  Pakaian, sepatu, tas, dan aksesoris untuk pria...\n",
      "2  Home Appliances  Peralatan rumah tangga seperti blender, vacuum...\n",
      "3            Books  Buku pendidikan, pengembangan diri, teknologi,...\n",
      "\n",
      "-- Missing Values:\n",
      "CategoryName    0\n",
      "Description     0\n",
      "dtype: int64\n",
      "\n",
      "==================== Orders Dataset ====================\n",
      "Shape: (3, 3)\n",
      "\n",
      "-- Sample Data:\n",
      "   UserID   OrderDate  TotalAmount\n",
      "0       1  2025-07-01   15670000.0\n",
      "2       3  2025-07-03    1270000.0\n",
      "3       4  2025-07-03    3695000.0\n",
      "\n",
      "-- Missing Values:\n",
      "UserID         0\n",
      "OrderDate      0\n",
      "TotalAmount    0\n",
      "dtype: int64\n",
      "\n",
      "==================== OrderItems Dataset ====================\n",
      "Shape: (7, 4)\n",
      "\n",
      "-- Sample Data:\n",
      "   OrderID  ProductID  Quantity       Price\n",
      "0        1          5         2     85000.0\n",
      "1        1          1         1  15500000.0\n",
      "2        3          6         1    320000.0\n",
      "4        4         10         1    145000.0\n",
      "5        3          8         1    950000.0\n",
      "\n",
      "-- Missing Values:\n",
      "OrderID      0\n",
      "ProductID    0\n",
      "Quantity     0\n",
      "Price        0\n",
      "dtype: int64\n",
      "\n",
      "==================== Payments Dataset ====================\n",
      "Shape: (3, 4)\n",
      "\n",
      "-- Sample Data:\n",
      "   OrderID     PaymentMethod PaymentDate      Amount\n",
      "0        1        Debit Card  2025-07-01  15670000.0\n",
      "2        3  Cash on Delivery  2025-07-03   1270000.0\n",
      "3        4         ShopeePay  2025-07-03   3695000.0\n",
      "\n",
      "-- Missing Values:\n",
      "OrderID          0\n",
      "PaymentMethod    0\n",
      "PaymentDate      0\n",
      "Amount           0\n",
      "dtype: int64\n",
      "\n",
      "==================== Reviews Dataset ====================\n",
      "Shape: (5, 4)\n",
      "\n",
      "-- Sample Data:\n",
      "   ProductID  UserID  Rating  \\\n",
      "0          1       2       5   \n",
      "1          2       1       4   \n",
      "2          3       5       5   \n",
      "3          4       3       4   \n",
      "4          5       4       3   \n",
      "\n",
      "                                          ReviewText  \n",
      "0  Laptop ini sangat cepat dan cocok untuk kerja ...  \n",
      "1                           Headphone nyaman dipakai  \n",
      "2  Smartwatch-nya membantu banget buat lacak olah...  \n",
      "3                                        Jaket keren  \n",
      "4                                       Kaos lumayan  \n",
      "\n",
      "-- Missing Values:\n",
      "ProductID     0\n",
      "UserID        0\n",
      "Rating        0\n",
      "ReviewText    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# list dataframe to be analyzed\n",
    "dataframes = {\n",
    "    \"Users\": users_df,\n",
    "    \"Products\": products_df,\n",
    "    \"Categories\": categories_df,\n",
    "    \"Orders\": orders_df,\n",
    "    \"OrderItems\": orderItems_df,\n",
    "    \"Payments\": payments_df,\n",
    "    \"Reviews\": reviews_df\n",
    "}\n",
    "\n",
    "# Loop EDA per dataset\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n==================== {name} Dataset ====================\")\n",
    "    print(\"Shape:\", df.shape)\n",
    "    print(\"\\n-- Sample Data:\")\n",
    "    print(df.head())\n",
    "\n",
    "    print(\"\\n-- Missing Values:\")\n",
    "    print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05467428-89c9-4101-9d09-54b6043164ff",
   "metadata": {},
   "source": [
    "# LOAD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84455f5-8bc0-4779-a49d-fbcbb12ef2d6",
   "metadata": {},
   "source": [
    "Load the transformed data into MySQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "918819a1-fa2d-4c26-b2cb-804bde1b9990",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_to_clear = ['OrderItems', 'Payments', 'Reviews', 'Orders', 'Products', 'Users', 'Categories']\n",
    "\n",
    "cursor.execute(\"SET FOREIGN_KEY_CHECKS = 0\")\n",
    "for table in tables_to_clear:\n",
    "    cursor.execute(f\"DELETE FROM {table}\")\n",
    "cursor.execute(\"SET FOREIGN_KEY_CHECKS = 1\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3d899b5c-d880-4365-996e-9037718ca2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ========== Load Categories ==========\n",
    "for index, row in categories_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Categories (CategoryName, Description)\n",
    "        VALUES (%s, %s)\n",
    "    \"\"\", (row['CategoryName'], row['Description']))\n",
    "conn.commit()\n",
    "\n",
    "### ========== Load Products ==========\n",
    "product_id_mapping = {}\n",
    "for index, row in products_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Products (ProductName, Description, Price, StockQuantity, Category)\n",
    "        VALUES (%s, %s, %s, %s, %s)\n",
    "    \"\"\", (row['ProductName'], row['Description'], row['Price'], row['StockQuantity'], row['Category']))\n",
    "    product_id_mapping[index] = cursor.lastrowid\n",
    "conn.commit()\n",
    "\n",
    "### ========== Load Users ==========\n",
    "user_id_mapping = {}\n",
    "for index, row in users_df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Users (UserName, Email, Address, Password)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (row['UserName'], row['Email'], row['Address'], row['Password']))\n",
    "    user_id_mapping[index] = cursor.lastrowid\n",
    "conn.commit()\n",
    "\n",
    "### ========== Load Orders ==========\n",
    "order_id_mapping = {}\n",
    "for index, row in orders_df.iterrows():\n",
    "    new_user_id = user_id_mapping.get(row['UserID'])\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Orders (UserID, OrderDate, TotalAmount)\n",
    "        VALUES (%s, %s, %s)\n",
    "    \"\"\", (new_user_id, row['OrderDate'], row['TotalAmount']))\n",
    "    order_id_mapping[index] = cursor.lastrowid\n",
    "conn.commit()\n",
    "\n",
    "### ========== Load OrderItems ==========\n",
    "for index, row in orderItems_df.iterrows():\n",
    "    new_order_id = order_id_mapping.get(row['OrderID'])\n",
    "    new_product_id = product_id_mapping.get(row['ProductID'])\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO OrderItems (OrderID, ProductID, Quantity, Price)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (new_order_id, new_product_id, row['Quantity'], row['Price']))\n",
    "conn.commit()\n",
    "\n",
    "### ========== Load Payments ==========\n",
    "for index, row in payments_df.iterrows():\n",
    "    new_order_id = order_id_mapping.get(row['OrderID'])\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO Payments (OrderID, PaymentMethod, PaymentDate, Amount)\n",
    "        VALUES (%s, %s, %s, %s)\n",
    "    \"\"\", (new_order_id, row['PaymentMethod'], row['PaymentDate'], row['Amount']))\n",
    "conn.commit()\n",
    "\n",
    "### ========== Load Reviews ==========\n",
    "for index, row in reviews_df.iterrows():\n",
    "    new_product_id = product_id_mapping.get(row['ProductID'])\n",
    "    new_user_id = user_id_mapping.get(row['UserID'])\n",
    "    if pd.notnull(new_product_id) and pd.notnull(new_user_id):\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO Reviews (ProductID, UserID, Rating, ReviewText)\n",
    "            VALUES (%s, %s, %s, %s)\n",
    "        \"\"\", (new_product_id, new_user_id, row['Rating'], row['ReviewText']))\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b18510-9e37-4dc9-bec2-76e5c1a4c5b8",
   "metadata": {},
   "source": [
    "# Challanges and Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188078c1-16a6-4dd2-9793-6c69ecefdf7f",
   "metadata": {},
   "source": [
    "During the ETL process, several challenges emerged:\n",
    "\n",
    "1. **Handling Embedded Commas in CSV Files**\n",
    "\n",
    "\n",
    "   Some category descriptions contained commas, which caused incorrect column splits when reading the CSV.\n",
    "\n",
    "\n",
    "   âœ… Solution: Applied proper handling using the quotechar parameter in pd.read_csv() and fallback parsing using string splitting when initial parsing failed.\n",
    "\n",
    "3. **Foreign Key Constraints on Loading**\n",
    "\n",
    "\n",
    "   Attempting to insert data into child tables (like Products or Orders) before parent tables (like Categories or Users) caused foreign key constraint errors.\n",
    "\n",
    "\n",
    "   âœ… Solution: Carefully ordered the loading sequenceâ€”Categories first, then Products, Users, Orders, OrderItems, Payments, and Reviewsâ€”to respect dependencies.\n",
    "\n",
    "5. **Resetting Auto-Increment IDs**\n",
    "\n",
    "\n",
    "   After deleting data, MySQL did not automatically reset AUTO_INCREMENT counters, resulting in non-sequential IDs.\n",
    "\n",
    "\n",
    "   âœ… Solution: Applied ALTER TABLE <table_name> AUTO_INCREMENT = 1 to reset ID counters after clearing data.\n",
    "\n",
    "7. **Rating and Numeric Data Issues**\n",
    "\n",
    "\n",
    "   Some rating values were improperly typed or out of the expected 1-5 range.\n",
    "\n",
    "\n",
    "   âœ… Solution: Cast ratings to integers and filtered invalid ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d464a37-df42-4486-9a93-7d490a232fcb",
   "metadata": {},
   "source": [
    "# Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e08e4-f4e9-4aa8-9663-f6b555b9a11c",
   "metadata": {},
   "source": [
    "1. **ETL Pipelines Require Careful Planning of Dependencies**\n",
    "\n",
    "The strict order in which data must be loaded due to foreign key relationships is crucial. Mapping old IDs to new ones helps maintain referential integrity.\n",
    "\n",
    "2. **Data Quality Upfront Minimizes Downstream Issues**\n",
    "\n",
    "Initial data had formatting inconsistencies that if not handled early, it would propagate errors throughout the pipeline.\n",
    "\n",
    "3. **Automation and Reusability**\n",
    "\n",
    "Encapsulating transformation steps in reusable functions would streamline future iterations on similar datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
